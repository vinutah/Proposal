\section{Thesis Statement}
Model compression is a ubiquitous tool that brings the power of modern deep learning 
to edge devices with power and latency constraints. The goal of model compression is 
to take a large reference neural network and output a smaller and less expensive 
compressed network that is functionally equivalent to the reference. Compression 
typically involves pruning and/or quantization, followed by executing accuracy recovery 
algorithms to maintain the reference accuracy. However, we observed that 
compression leads to a considerable mismatch in the labels produced by the reference and 
the compressed models, resulting in bias and unreliability. 
To combat this, In this thesis  we present user programmable systems for resource efficient 
model compression and methods to improve its reliability so that it can also be used for mission-critical  domains.